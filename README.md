# IJEPA-Diffusion-GAN ğŸ©ºâœ¨  
*A hybrid generative framework for anatomyâ€‘aware, dataâ€‘efficient medicalâ€‘image synthesis*

![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![Papers](https://img.shields.io/badge/paper-Purdue%20M.S.%20Thesis-orange)

> **TL;DR**â€‚IJEPAâ€‘Diffusionâ€‘GAN fuses selfâ€‘supervised **Iâ€‘JEPA** semantic embeddings with the fidelity of diffusion models **and** the speed of StyleGANâ€‘family adversarial refinements.  
> Compared with pixelâ€‘space diffusion it **cuts GPU memory by â‰ˆ48â€¯%** while matching or beating stateâ€‘ofâ€‘theâ€‘art FID/KID on chestâ€‘Xâ€‘ray & brainâ€‘MRI benchmarks.

---

## 1â€‚Why this repo existsÂ ğŸ¤”  

Medical imaging suffers from a threeâ€‘way trilemma: **fidelity, diversity, efficiency**.  
Traditional GANs collapse; pure diffusion is computeâ€‘hungry.  
Our thesis shows that injecting *semantic guidance* from Iâ€‘JEPA into a diffusionâ€‘GAN pipeline offers a balanced remedy.

---

## 2â€‚Key featuresÂ ğŸš€  

| ğŸ”‘  | What it does | Where to look |
|-----|--------------|---------------|
| **JEPAFusionMapping** | AdaINâ€‘style latent injection of 2â€¯048â€‘D Iâ€‘JEPA codes into StyleGAN2â€‘ADA stages | `models/ijepa_fusion.py` |
| **Diffusionâ€‘StyleGAN2 backbone** | DDPM steps inside StyleGAN2 discriminator for multiscale denoising | `models/diff_sg2/` |
| **Configâ€‘driven experiment grid** | 3 backbones Ã— 3 conditioning schemes Ã— 2 datasets, fully reproducible | `configs/*.yaml`, `scripts/train.sh` |
| **Metrics & dashboards** | FIDâ€‘50k, KID, IS, Precision/Recall, W&B logging | `metrics/`, W&B link |

---

## 3â€‚Project treeÂ ğŸŒ²  

```text
.
â”œâ”€â”€ calc_metrics_pr.py
â”œâ”€â”€ calc_metrics.py
â”œâ”€â”€ calc_metrics.sh
â”œâ”€â”€ dataset_tool.py
â”œâ”€â”€ environment.yml
â”œâ”€â”€ gen_images.py
â”œâ”€â”€ gen_video.py
â”œâ”€â”€ generate_samples.sh
â”œâ”€â”€ ijepa-film-stylegan2_gpu_4.sh
â”œâ”€â”€ ijepa-film-stylegan2_gpu_4_brain.sh
â”œâ”€â”€ legacy.py
â”œâ”€â”€ train.py
â”œâ”€â”€ diffusion-projected-lossonly-gan/
â”œâ”€â”€ diffusion-projected-ramp-gan/
â”œâ”€â”€ diffusion-ramp-stylegan2/
â”œâ”€â”€ ijepa-diffusion-lossonly-stylegan2/
â”œâ”€â”€ ijepa-lossonly-stylegan2/
â”œâ”€â”€ ijepa-ramp-stylegan2/
â”œâ”€â”€ dnnlib/
â”œâ”€â”€ metrics/
â”œâ”€â”€ pg_modules/
â”œâ”€â”€ torch_utils/
â””â”€â”€ training/
    â”œâ”€â”€ dataset.py
    â”œâ”€â”€ ijepa_encoder.py
    â”œâ”€â”€ loss.py
    â””â”€â”€ training_loop.py
```

---

## 4â€‚Setup & trainingÂ ğŸ”§  

### 4.1â€‚Local environment (optional)  
Use this for **dataset conversion**, **metric evaluation**, or lightweight debugging.

```bash
conda create -n ijepa_diffusion_gan python=3.10      # â¶ create env
conda activate ijepa_diffusion_gan
pip install -r requirements.txt                      # â· install deps

# download pretrained Iâ€‘JEPA backbone (~120â€¯MB)
bash scripts/download_ijepa.sh                       # â¸
```

*Training the full model requires multiâ€‘GPU â€“ see next section.*

### 4.2â€‚HPC (SLURMâ€¯+â€¯Apptainer) training  
Each folder in `scripts/StyleGAN2/*` ships with a readyâ€‘toâ€‘submit SLURM file like below
(`ijepa_SG_Chest_rampGD_warmup_5.4_4gpu_sem_mixing_0.9_FusionAlpha_0.2.sh`):

```bash
#!/bin/bash

export PROJECT=~/StyleGAN2/stylegan2-ada-pytorch
export TMPDIR=$PROJECT/tmp/torch_tmp
mkdir -p "$TMPDIR"

SEM_MIX=0.9
FUSION_ALPHA=0.2

srun apptainer exec --nv --cleanenv \
    -B $PROJECT:$PROJECT \
    $PROJECT/SIF_FILES/stylegan2ada-devel.sif \
    /opt/conda/bin/python \
    $PROJECT/ijepa-ramp-stylegan2/train.py \
        --outdir=$PROJECT/outputs/Chest/sem_mixing_${SEM_MIX}/run_%j \
        --data=/scratch/<user>/dataset/256/chest_xray_labelled.zip \
        --gpus=4 --cond=1 \
        --ijepa_checkpoint /scratch/<user>/ijepa_backbone.pth \
        --ijepa_lambda 1.0 --ijepa_image 256 \
        --extra_dim 2048 --ijepa_warmup_kimg 5.4 \
        --sem_mixing_prob ${SEM_MIX} --fusion_alpha ${FUSION_ALPHA}
```

**Steps to launch**

1. Edit dataset paths / checkpoint paths.
2. Pick your `SEM_MIX` and `FUSION_ALPHA`.
3. Submit â†’ `sbatch scripts/StyleGAN2/ijepa-ramp-stylegan2/your_job.sh`.

The script will:
* spin up 1â€¯nodeâ€¯Ã—â€¯4â€¯GPUs  
* mount the project into the Apptainer container (`stylegan2ada-devel.sif`)  
* resume from `--resume` if provided, else start fresh  
* write snapshots / metrics in `outputs/â€¦/training-runs/`  

*Experiments in the thesis ran on 4Ã—Â A100Â (32â€¯GB); singleâ€‘GPU training is possible with `--batch_gpu 8`.*

### 4.3â€‚Evaluation & Metrics (SLURM)

To compute Precision/Recall, KID, and IS on your trained checkpoints via SLURMâ€‰+â€‰Apptainer, create a job script like below (`scripts/StyleGAN2/ijepa-ramp-stylegan2/metrics_job.sh`):

```bash
#!/bin/bash

# 1ï¸âƒ£  Precision/Recall @50k (conditional)
srun --gres=gpu:2 \
  apptainer exec --nv --cleanenv \
    -B $PROJECT:$PROJECT \
    $PROJECT/SIF_FILES/stylegan2ada-devel.sif \
    python /scratch/<user>~/calc_metrics_pr.py \
      --metrics=pr50k3_full_cond \
      --network=/scratch/<user>~/network-snapshot-000403.pkl \
      --data=/scratch/<user>~/dataset/256/Brain_cancer_labelled.zip

# 2ï¸âƒ£  KID & IS @50k
srun --gres=gpu:2 \
  apptainer exec --nv --cleanenv \
    -B $PROJECT:$PROJECT \
    $PROJECT/SIF_FILES/stylegan2ada-devel.sif \
    python /scratch/<user>~/calc_metrics.py \
      --metrics=kid50k_full,is50k \
      --network=/scratch/<user>~/network-snapshot-000403.pkl \
      --data=/scratch/<user>~/dataset/256/Brain_cancer_labelled.zip
```
---

## 5â€‚Results snapshotÂ ğŸ“Š  

| Model (ChestÂ Xâ€‘rayÂ 256Â²) | FIDÂ â†“ | KIDÂ Ã—10Â³Â â†“ | ISÂ â†‘ |
|--------------------------|-------|------------|------|
| StyleGAN2â€‘ADA baseline   | **5.63** | 3.2 | 2.32 |
| **Diffâ€‘Projâ€‘FastGANÂ +Â JEPA** | 3.76 | **0.4** | 2.30 |
| Diffâ€‘StyleGAN2 baseline  | 10.09 | 8.3 | 2.31 |

*(Complete tables and perâ€‘class precision/recall in the thesis, SectionÂ 6.)*

---

## 6â€‚Reproducing the thesisÂ ğŸ“–  

Exact code, seeds `[2025,Â 425,Â 9001]`, and SLURM scripts used for every figure and table are archived **here** â†’ <https://github.com/08Abhinay/IJEPA-Diffusion-GAN>.  
Run:

```bash
make all   # generates every figure & metric in ~18â€¯h on 4â€¯GPUs
```

---

## 7â€‚CitationÂ ğŸ“  

```bibtex
@article{Belde2025,
author = "Abhinay Shankar Belde",
title = "{Addressing Data Scarcity in Medical Imaging: A Hybrid Approach Combining IJEPA, Diffusion, and GANs}",
year = "2025",
month = "7",
url = "https://hammer.purdue.edu/articles/thesis/Addressing_Data_Scarcity_in_Medical_Imaging_A_Hybrid_Approach_Combining_IJEPA_Diffusion_and_GANs/29649557",
doi = "10.25394/PGS.29649557.v1"
}
```


---


## 8â€‚AcknowledgmentsÂ ğŸ™  

Thanks to **Dr. MohammadrezaÂ Hajiarbabi**, **Dr.Â JonathanÂ Rusert**, and **Dr.Â AlessandroÂ Selvitella** for invaluable guidance, and to Purdueâ€™s Gilbreth HPC staff for compute support.

This work builds upon several outstanding open-source repositories and their authors:
- **StyleGAN2-ADA** by Karras et al. (https://github.com/NVlabs/stylegan2-ada)  
- **InsGen** by Yang et al. (https://github.com/ceyuanyang/InsGen)  
- **ProjectedGAN** by Sauer et al. (https://github.com/axelsauer/projected-gan)  
- **Diffusion-GAN** by Wang et al. (https://github.com/Zhendong-Wang/Diffusion-GAN)

